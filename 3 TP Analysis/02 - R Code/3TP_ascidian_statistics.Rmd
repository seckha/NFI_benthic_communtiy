---
title: "3 TP Ascidian statistics"
author: "Shannon Eckhardt"
date: "2023-10-31"
output: html_document
---

Statistics for ascidian cover

# IMPORT LIBRARIES
```{r}
rm(list=ls())
library(lme4)
library(boot)
library(readxl) # load data
library(glmmTMB) # for hurdle model
library(emmeans) # for post-hoc analysis
library(lmtest) # for likelihood ratio test
library(DHARMa) # for diagnostics
library(tidyverse)
```

# IMPORT DATA
```{r}
ascd.transect.percent.cover <- read_excel("/Users/Shannon/GitHub/NFI_benthic_community/3 TP Analysis/01 - Data/Ascidian_transect_percent_cover.xlsx")

full_data <- read_excel("/Users/Shannon/GitHub/NFI_benthic_community/3 TP Analysis/01 - Data/FULL_DATA_21_22_23.xlsx")

# transect percent cover
full_dat_transect_percent_cover <- full_data %>%
  group_by(TP, Site, Transect_ID, Category) %>%
  summarize(percent_cover = mean(sum_count)) %>% # mean() already gives a percentage
  mutate(SE = sd(percent_cover) / sqrt(n()))

# make tibble into data frame
full_dat_transect_percent_cover <- as.data.frame(full_dat_transect_percent_cover)

# site percent cover
full_dat_site_percent_cover <- full_data %>%
  group_by(TP, Site, Category) %>%
  summarize(percent_cover = mean(sum_count)) %>% # mean() already gives a percentage
  mutate(SE = sd(percent_cover) / sqrt(n()))
```

# HURDLE MODEL - using glmmTMB
```{r}
# convert percent cover from 0-100 to 0-1
ascd.transect.percent.cover$percent_cover <- ascd.transect.percent.cover$percent_cover / 100

m.hurd <- glmmTMB(percent_cover ~ TP * Site + (1|Transect_ID), ziformula = ~1, data = ascd.transect.percent.cover, family=beta_family())


# diagnostics
simulationOutput <- simulateResiduals(fittedModel = m.hurd, plot = F)
plot(simulationOutput)
```

# ANOVA/LRT - using lmtest
Does ascidian cover significantly vary between site and time point?

To do a LRT I usually test by fitting the null and alternative model and then run anova() on the two models. This is testing if the change in percent of cover is different for sites over time, i.e., the interaction term. That is that slopes are different for sites over time. It is also testing if the alternative model is better at explaining the data than the null model. 
```{r}
# Likelihood ratio test (LRT) instead of anova as they are more suited for mixed-effects models and models with non-Gaussian distributions?

# Model 1 (my model)
m.hurd <- glmmTMB(percent_cover ~ TP * Site + (1|Transect_ID), ziformula = ~1, data = ascd.transect.percent.cover, family=beta_family())

# Model 2 (null hypothesis - no interaction between TP & Site)
m.hurd.null <- glmmTMB(percent_cover ~ TP + Site + (1|Transect_ID), ziformula = ~1, data = ascd.transect.percent.cover, family=beta_family())

anova(m.hurd.null, m.hurd)

# Is this telling me that the null hypothesis can be rejected as p < 0.001?
# This would mean that the interaction term of TP*Site is a better model = they significantly influence percent cover?

```
# POST-HOC ANALYSIS

The emmip function is a really easy way to visualise the interaction term.
```{r}
emmeans_plot <- emmip(m.hurd,  ~ TP ~ Site, CIs = TRUE) 
emmeans_plot
```

To set up the contrasts of the interaction terms, I first get the estimated marginal means for all of the combinations of the factor levels as follows:

```{r}
em_height <- emmeans(m.hurd,  ~ TP + Site, combine= TRUE)
```

Then I store the order of the factors that emmeans uses:
```{r}
combm <-  as.data.frame(em_height)[,1:2]
```

I create a diagonal matrix which has the same number of rows and columns as the number of combinations (or levels if it only one factor). There are 3 levels in each of height_cat and YEAR, so the matrix is as follows:
```{r}
base <- as.data.frame(diag(3*3))
```
Then you can name the rows and columns to be the combination levels
```{r}
colnames(base) = rownames(base) = paste0(combm$TP, combm$Site)
base

```

Then I can calculate the differences in change between time points between levels of height. For example, looking at the differences of change for High and Medium between 96 and 95 is coded as (base$High96 - base$High95) - (base$Med96 - base$Med95). To adjust for multiple pairwise tests use the multivariate t distribution adjust = "mvt".

```{r}
selected_contrasts <- contrast(em_height, method = 
                                    # is the rate of change different within sites over time (between years)
                                 list("CB 22-21 v. CB 23-22" = (base$`April 2022CB` - base$`April 2021CB`) - (base$`April 2023CB` - base$`April 2022CB`),
                                      "EB 22-21 v. EB 23-22" = (base$`April 2022EB` - base$`April 2021EB`) - (base$`April 2023EB` - base$`April 2022EB`),
                                      "SB 22-21 v. SB 23-22" = (base$`April 2022SB` - base$`April 2021SB`) - (base$`April 2023SB` - base$`April 2022SB`), 
                               
                                      # is the rate of change different between CB and EB between years
                                      "CB 22-21 v. EB 22-21" = (base$`April 2022CB` - base$`April 2021CB`) - (base$`April 2022EB` - base$`April 2021EB`),
                                      "CB 23-22 v. EB 23-22" = (base$`April 2023CB` - base$`April 2022CB`) - (base$`April 2023EB` - base$`April 2022EB`),
                                      
                                      # is the rate of change different between CB and SB between years
                                      "CB 22-21 v. SB 22-21" = (base$`April 2022CB` - base$`April 2021CB`) - (base$`April 2022SB` - base$`April 2021SB`),
                                      "CB 23-22 v. SB 23-22" = (base$`April 2023CB` - base$`April 2022CB`) - (base$`April 2023SB` - base$`April 2022SB`),
                                      
                                      # is the rate of change different between EB and SB between years
                                      "EB 22-21 v. SB 22-21" = (base$`April 2022EB` - base$`April 2021EB`) - (base$`April 2022SB` - base$`April 2021SB`),
                                      "EB 23-22 v. SB 23-22" = (base$`April 2023EB` - base$`April 2022EB`) - (base$`April 2023SB` - base$`April 2022SB`)),
                               adjust = "mvt", type = "response")
```

Results of the pairwise test are as follows:
```{r}
cbind(summary(selected_contrasts),confint(selected_contrasts)[,5:6]) %>% 
  as.data.frame() %>% 
  mutate_at(2:8, round, 3)
```

1. Suggested evidence that there is a difference in change of ascidian cover between CB 22-21 to CB 23-22
2. Suggested evidence that there is a difference in change of ascidian cover between CB 23-22 to EB 23-22
3. Suggested evidence that there is a difference in change of ascidian cover between CB 23-22 to SB 23-22

*This means that compared to 2021-2022 in CB, and 2022-2023 in SB and CB, there is suggested evidence that the change of ascidian cover in CB from 2022-2023 is different. Meaning that in CB from 2022-2023, ascidian cover did not increase at the same level as EB and SB in the same year.*


The results confirm what was shown in the interaction plot. Two pairwise comparisons, H v. L by time 96-95 and M v. L by time 96-95 suggested evidence there was a difference in change between 96 and 95 across heights.











# OLD - not used for the analysis
## POST-HOC ANALYSIS - using emmeans
```{r}
# calculate marginal means for Site and Time Point
emmeans_results <- emmeans(m.hurd, ~ Site + TP, model = "zero_infl")

# perform pairwise comparisons for sites
site_comparisons <- pairs(emmeans_results, by = "Site")

# perform pairwise comparisons for time points
tp_comparisons <- pairs(emmeans_results, by = "TP")

# perform pairwise comparisons for both Site and TP
site_tp_comparisons <- pairs(emmeans_results, by = NULL)

# summarize the results
summary(site_comparisons)
summary(tp_comparisons)
summary(site_tp_comparisons)
```

# TRY TO DO PCA

after: https://www.youtube.com/watch?v=mNpBrHwOCt4

```{r}
library(MASS)
library(factoextra)
library(ggplot2)

# transpose df
wide.full.dat <- full_dat_transect_percent_cover %>% pivot_wider(names_from = Category, values_from = percent_cover)

str(wide.full.dat) # all relevant variables are numeric
summary(wide.full.dat)

# delete classes with NA
# have no NAs

# exclude categorical data (in 4 four columns)
full.dat.sample <- wide.full.dat[, -c(1:4)]

# run PCA
full.dat.pca <- prcomp(full.dat.sample,
                       scale = TRUE) # ensures that data is standardized - important to avoid bias in analysis

# summary of analysis
summary(full.dat.pca)


# elements of PCA object
names(full.dat.pca)
# 5 more elements provide additional analysis output

# standard dev of components
full.dat.pca$sdev

# eigenvectors = loadings per variable per vector component
full.dat.pca$rotation

# standard dev & mean of original variables
full.dat.pca$center
full.dat.pca$scale

# principal component scores for all 9 components
full.dat.pca$x



# VISUALIZE RESULTS

# scree plot of variance = shows explained variance per component - used to decide on optimal number of components to retain in analysis
fviz_eig(full.dat.pca, 
         addlabels = TRUE, # visualize variance percentages
         ylim = c(0, 40)) # to arrange limits of y axis

# biplot with default settings - interpret PCA results
fviz_pca_biplot(full.dat.pca)

# biplot with labeled variables = suppress data point labels
fviz_pca_biplot(full.dat.pca,
                label = "var")

# biplot with colored groups - color by a group = TP or site
fviz_pca_biplot(full.dat.pca,
                label = "var",
                habillage = wide.full.dat$Site)

fviz_pca_biplot(full.dat.pca,
                label = "var",
                habillage = wide.full.dat$TP)

# biplot with customized colored groups & variables
fviz_pca_biplot(full.dat.pca,
                label = "var",
                habillage = wide.full.dat$Site, 
                col.var = "black") +
  scale_color_manual(values = c("magenta", "orange", "steelblue"))

fviz_pca_biplot(full.dat.pca,
                label = "var",
                habillage = wide.full.dat$TP, 
                col.var = "black") +
  scale_color_manual(values = c("magenta", "orange", "steelblue"))

```


# ORDINATION PLOT: NMDS ANALYSIS
```{r}
library(vegan)

# transpose df
wide.full.dat <- full_dat_transect_percent_cover %>% pivot_wider(names_from = Category, values_from = percent_cover)

# exclude categorical data (in 4 four columns)
full.dat.sample <- wide.full.dat[, -c(1:4)]
# turn tibble into df
full.dat.sample <- as.data.frame(full.dat.sample)
str(wide.full.dat)
# 1 - Calculate dissimilarity matrix
  # quantify dissimilarity or similarity between sites based on benthic community data
diss_matrix <- vegdist(full.dat.sample, method = "bray")

# 2 - Perform NMDS
set.seed(1)
nmds_result <- metaMDS(diss_matrix)

# 3 - Visualize NMDS plot
plot(nmds_result)

# Stress plot
  # if all the blue points fall on red line = low stress value
  # lower stress value = better NMDS model
stressplot(nmds_result)
nmds_result$stress # stress value
```
How do you decide if a stress value is good or not? -> *Goodness-of-fit*
Rule of thumb:
Stress value *> 2* indicates a *poor fit* -> risks in interpretation
Stress value *0.1 - 0.2* indicates a *fair fit* -> some distances misleading
Stress value *0.05-0.1* indicates a *good fit* -> inferences confident
Stress value *< 0.05* indicates *excellent fit*

If you do ordination in more dimensions (more than 2 like we are doing here), the stress value decreases and the fit increases as more dimensions can explain the distances between points better

```{r}
# make the NMDS plot in ggplot to visualize better
# need to extract x and y axes from nmds_result = NMDS1 & NMDS2
nmdsscores <- as.data.frame(scores(nmds_result))

```

## SECOND TRY
```{r}
# use site percent cover (because transect ID irrelevant)
wide.site.cover <- full_dat_site_percent_cover %>% pivot_wider(names_from = Category, values_from = percent_cover)

# for ndms, character columns need to be excluded - exclude TP and Site & then Category will be the rownames
sample <- wide.site.cover[, -c(1:3)]

diss_matrix <- vegdist(sample, method = "bray")

set.seed(1)
nmds <- metaMDS(diss_matrix)

nmds$points
scores(nmds)
plot(nmds)

# Get the column (Category) names
category_names <- colnames(sample)

# Convert NMDS result to a data frame
nmds_data <- data.frame(NMDS1 = nmds$points[, 1], NMDS2 = nmds$points[, 2])

# Create the NMDS plot using ggplot
ggplot(nmds_data, aes(NMDS1, NMDS2)) +
  geom_point() +
  geom_text(aes(label = category_names), nudge_x = 0.02, nudge_y = 0.02, size = 3) +
  theme_bw()

# here sample has 9 rows and 9 columns so the points actually display the 3 sites at the 3 time points and not the 9 categories!!!
```



# CORRELATION MATRIX

Look for correlations between variables

```{r}
library(corrplot)

str(wide.full.dat)
res <- cor(wide.full.dat[,c(5:13)])
round(res, 2)

# corrplot(res)
# corrplot(
#   res,
#   method = "color",
#   col.lab = "black",   # Color of numeric labels
#   tl.col = "black",    # Color of top labels
#   cl.col = "black",    # Color of left labels
#   number.cex = 1       # Set the size of numeric labels
# )
# 
# palette = colorRampPalette(c("skyblue", "white", "salmon")) (10)
# heatmap(x = res, col = palette, symm = TRUE)


# RUN THE BOTTOM CHUNK ALL IN ONE - CREATE PNG - PLOT - CLOSE GRAPHICS DEVICE

# Set up a PNG graphics device with smaller pointsize
png("/Users/Shannon/GitHub/NFI_benthic_community/3 TP Analysis/03 - Plots/correlation_plot.png", width = 800, height = 800, units = "px", res = 300, pointsize = 8)

# Create correlation plot with adjusted sizes
corrplot(
  res,
  method = "color",
  col = colorRampPalette(c("skyblue", "white", "salmon"))(10),
  type = "upper",
  order = "hclust",
  addCoef.col = "black",
  tl.col = "black",
  diag = FALSE,
  number.cex = 0.4,  # Adjusted size of correlation numbers
  tl.cex = 0.6,     # Size of top labels
  cl.cex = 0.6,     # Size of left labels
  mar = c(1, 1, 1, 1),  # Adjust the margin
  col.lab = "black",  # Color of numeric labels
  cex.axis = 0.8,  # Adjusted size of the squares
  cl.ratio = 0.2,  # Adjust the size of the color bar in the legend
  cl.offset = 1  # Adjust the offset of the color bar in the legend
)

# Close the PNG graphics device
dev.off()
```
Interesting:
- Hard coral & macroalgae: -0.49 = when there's more coral there's less macroalgae
- Hard coral & benthic inverts: 0.37 = when there's more hard coral, there's more benthic inverts
- Macroalgae & hard coral: -0.49 = when there's more macroalgae there's less coral?

Not really any strong correlations for ascidians at all
-> highest one is ascidians & macroalgae with -0.1 = more ascidians, less macro & more macro, less ascidians?



